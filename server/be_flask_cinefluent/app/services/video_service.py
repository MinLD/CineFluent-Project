import yt_dlp
from slugify import slugify
from ..models.models_model import Video, Subtitle, Category
from ..extensions import db
from ..schemas.video_schema import VideoSchema


def fetch_youtube_metadata(url):
    """S·ª≠ d·ª•ng yt-dlp ƒë·ªÉ l·∫•y Title v√† Thumbnail t·ª´ link YouTube."""
    ydl_opts = {'quiet': True, 'no_warnings': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=False)
        return {
            'title': info.get('title'),
            'thumbnail': info.get('thumbnail'),
            'duration': info.get('duration'),
            'youtube_id': info.get('id'),
            'description': info.get('description')
        }
def delete_video_youtube(id):
    video = Video.query.get(id)
    if not video:
        raise ValueError('Video id {} not found'.format(id))
    db.session.delete(video)
    db.session.commit()
def create_unique_slug(model, base_title, max_length=100):
    base_slug = slugify(base_title)[:max_length]
    slug = base_slug
    counter = 1
    while model.query.filter_by(slug=slug).first():
        suffix = f"-{counter}"
        slug = f"{base_slug[:max_length - len(suffix)]}{suffix}"
        counter += 1

    return slug


def import_youtube_video(url, level, user_id):
    meta = fetch_youtube_metadata(url)
    yt_id = meta['youtube_id']

    # 1. X·ª≠ l√Ω Category


    # 2. Ki·ªÉm tra Video t·ªìn t·∫°i ch∆∞a
    video = Video.query.filter_by(youtube_id=yt_id).first()
    if video:
        return video

    try:
        import os
        import yt_dlp
        import json
        import tempfile

        # T√¨m file cookies
        current_dir = os.path.dirname(os.path.abspath(__file__))
        cookie_path = os.path.join(current_dir, "../utils/www.youtube.com_cookies.txt")
        cookie_path = os.path.abspath(cookie_path)

        # T·∫°o th∆∞ m·ª•c t·∫°m
        temp_dir = tempfile.mkdtemp()

        # C·∫•u h√¨nh yt-dlp
        ydl_opts = {
            'skip_download': True,
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['en', 'vi'],
            'subtitlesformat': 'json3',
            'outtmpl': os.path.join(temp_dir, '%(id)s.%(ext)s'),
            'quiet': True,
            'no_warnings': True,
        }

        if os.path.exists(cookie_path):
            ydl_opts['cookiefile'] = cookie_path
            print(f"Using cookies")

        # Download subtitles v√† l·∫•y metadata
        print(f"Downloading subtitles for: {url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # Extract info ƒë·ªÉ l·∫•y metadata
            info = ydl.extract_info(url, download=False)

            # Ki·ªÉm tra subtitle metadata
            subtitles = info.get('subtitles', {})
            automatic_captions = info.get('automatic_captions', {})

            # Log th√¥ng tin subtitle
            print("\nSubtitle Information:")
            print(f"   Manual subtitles available: {list(subtitles.keys())}")
            print(f"   Auto-generated captions available: {list(automatic_captions.keys())}")

            # Ki·ªÉm tra ti·∫øng Anh
            if 'en' in subtitles:
                print(f"   English: MANUAL (human-created)")
                en_type = "manual"
            elif 'en' in automatic_captions:
                print(f"   English: AUTO-GENERATED")
                en_type = "auto"
            else:
                print(f"   English: NOT FOUND")
                en_type = "none"

            # Ki·ªÉm tra ti·∫øng Vi·ªát
            if 'vi' in subtitles:
                print(f"   Vietnamese: MANUAL (human-created)")
                vi_type = "manual"
            elif 'vi' in automatic_captions:
                print(f"   Vietnamese: AUTO-GENERATED")
                vi_type = "auto"
            else:
                print(f"   Vietnamese: NOT FOUND (will use auto-translate)")
                vi_type = "translated"

            print()  # Empty line for readability

            # Download subtitles
            ydl.download([url])

        # Parse subtitle files
        def parse_json3_file(filepath):
            """Parse subtitle t·ª´ file json3"""
            if not os.path.exists(filepath):
                return []

            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            result = []
            for event in data.get('events', []):
                if 'segs' in event and event.get('segs'):
                    text = ''.join(seg.get('utf8', '') for seg in event['segs'])
                    if text.strip():
                        result.append({
                            'text': text.strip(),
                            'start': event.get('tStartMs', 0) / 1000.0,
                            'duration': event.get('dDurationMs', 0) / 1000.0
                        })
            return result

        # T√¨m file subtitle ƒë√£ download
        en_file = os.path.join(temp_dir, f"{yt_id}.en.json3")
        vi_file = os.path.join(temp_dir, f"{yt_id}.vi.json3")

        # N·∫øu kh√¥ng c√≥ .en.json3, th·ª≠ t√¨m .en-*.json3
        if not os.path.exists(en_file):
            import glob
            en_files = glob.glob(os.path.join(temp_dir, f"{yt_id}.en*.json3"))
            if en_files:
                en_file = en_files[0]

        if not os.path.exists(vi_file):
            import glob
            vi_files = glob.glob(os.path.join(temp_dir, f"{yt_id}.vi*.json3"))
            if vi_files:
                vi_file = vi_files[0]

        print(f"Temp directory: {temp_dir}")
        print(f"English file: {en_file} (exists: {os.path.exists(en_file)})")
        print(f"Vietnamese file: {vi_file} (exists: {os.path.exists(vi_file)})")

        # Parse English subtitles
        transcript = parse_json3_file(en_file)
        if not transcript:
            raise ValueError("Kh√¥ng t√¨m th·∫•y ph·ª• ƒë·ªÅ ti·∫øng Anh")

        print(f"Got {len(transcript)} English entries ({en_type})")

        # D·ªãch subtitle b·∫±ng Google Translate API (BATCH MODE - T·ªëi ∆∞u ƒë·ªông)
        print("Translating subtitles using Google Translate (Dynamic Batch mode)...")
        
        from deep_translator import GoogleTranslator
        import time
        
        translator = GoogleTranslator(source='en', target='vi')
        
        # T√≠nh batch size ƒë·ªông d·ª±a tr√™n ƒë·ªô d√†i text
        total_chars = sum(len(item['text']) for item in transcript)
        avg_chars = total_chars / len(transcript) if transcript else 50
        
        MAX_CHARS_PER_BATCH = 4500  # An to√†n d∆∞·ªõi gi·ªõi h·∫°n 5000
        SEPARATOR = ' |||SUBTITLE_SEP||| '
        SEPARATOR_LENGTH = len(SEPARATOR)
        
        # T√≠nh batch size t·ªëi ∆∞u
        BATCH_SIZE = int(MAX_CHARS_PER_BATCH / (avg_chars + SEPARATOR_LENGTH))
        BATCH_SIZE = max(10, min(BATCH_SIZE, 200))  # Gi·ªõi h·∫°n 10-200
        
        print(f"   üìä Total subtitles: {len(transcript)}")
        print(f"   üìè Average chars per subtitle: {avg_chars:.1f}")
        print(f"   üéØ Optimal batch size: {BATCH_SIZE}")
        
        transcript_vi = []
        total_batches = (len(transcript) + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"   üì¶ Total batches: {total_batches}")
        
        def translate_with_retry(text, max_retries=3):
            """D·ªãch v·ªõi retry logic cho rate limiting"""
            for attempt in range(max_retries):
                try:
                    return translator.translate(text)
                except Exception as e:
                    error_str = str(e)
                    # Ki·ªÉm tra rate limiting
                    if '429' in error_str or 'Too Many Requests' in error_str or 'rate' in error_str.lower():
                        if attempt < max_retries - 1:
                            wait_time = (2 ** attempt) * 2  # Exponential backoff: 2s, 4s, 8s
                            print(f"      ‚ö†Ô∏è Rate limited, waiting {wait_time}s before retry...")
                            time.sleep(wait_time)
                        else:
                            raise Exception(f"Rate limit exceeded after {max_retries} retries")
                    else:
                        raise
            raise Exception("Translation failed after max retries")
        
        for batch_idx in range(0, len(transcript), BATCH_SIZE):
            batch_num = (batch_idx // BATCH_SIZE) + 1
            batch = transcript[batch_idx:batch_idx + BATCH_SIZE]
            
            # T√≠nh s·ªë k√Ω t·ª± trong batch n√†y
            batch_chars = sum(len(item['text']) for item in batch)
            
            try:
                # G·ªôp t·∫•t c·∫£ c√¢u trong batch th√†nh 1 string
                combined_text = SEPARATOR.join([item['text'] for item in batch])
                
                print(f"   üì¶ Batch {batch_num}/{total_batches}: {len(batch)} subtitles ({batch_chars} chars)...")
                
                # D·ªãch 1 l·∫ßn cho c·∫£ batch v·ªõi retry
                translated_combined = translate_with_retry(combined_text)
                
                # T√°ch l·∫°i th√†nh c√°c c√¢u ri√™ng l·∫ª
                translated_parts = translated_combined.split('|||SUBTITLE_SEP|||')
                
                # Clean up whitespace
                translated_parts = [part.strip() for part in translated_parts]
                
                # Ki·ªÉm tra s·ªë l∆∞·ª£ng c√≥ kh·ªõp kh√¥ng
                if len(translated_parts) != len(batch):
                    print(f"      ‚ö†Ô∏è Warning: Expected {len(batch)} parts, got {len(translated_parts)}")
                    # Fallback: d·ªãch t·ª´ng c√¢u n·∫øu batch translation fail
                    print(f"      üîÑ Falling back to single-sentence translation...")
                    translated_parts = []
                    for item in batch:
                        try:
                            vi_text = translate_with_retry(item['text'])
                            translated_parts.append(vi_text)
                        except:
                            translated_parts.append('')
                
                # Th√™m v√†o k·∫øt qu·∫£
                for i, item in enumerate(batch):
                    vi_text = translated_parts[i] if i < len(translated_parts) else ''
                    transcript_vi.append({
                        'text': vi_text,
                        'start': item['start'],
                        'duration': item['duration']
                    })
                
                print(f"      ‚úÖ Batch {batch_num}/{total_batches} completed!")
                
                # Th√™m delay nh·ªè gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limit
                if batch_num < total_batches:
                    time.sleep(0.5)
                
            except Exception as e:
                print(f"      ‚ùå Batch {batch_num} failed: {str(e)}")
                print(f"      üîÑ Falling back to single-sentence translation...")
                
                # Fallback: d·ªãch t·ª´ng c√¢u
                for item in batch:
                    try:
                        vi_text = translate_with_retry(item['text'])
                    except:
                        vi_text = ''
                    
                    transcript_vi.append({
                        'text': vi_text,
                        'start': item['start'],
                        'duration': item['duration']
                    })
                    time.sleep(0.3)  # Delay nh·ªè gi·ªØa c√°c c√¢u
        
        print(f"‚úÖ Translated {len(transcript_vi)}/{len(transcript)} subtitles successfully (Dynamic Batch mode)")

        # Cleanup temp files
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

        from .learning_service import suggest_video_category

        # 3. X·ª≠ l√Ω Category (AI Suggestion) - moved here to save tokens
        print(f"Asking AI to categorize: {meta['title']}...")
        ai_result = suggest_video_category(meta['title'], meta.get('description', ''))
        
        if ai_result.get('success'):
            category_name = ai_result['data'].get('category', 'General')
            print(f"   AI Suggested: {category_name}")
        else:
            print(f"   AI Error, fallback to General. Error: {ai_result.get('error')}")
            category_name = 'General'

        # T·∫°o Slug t·ª´ t√™n AI g·ª£i √Ω
        category_slug = slugify(category_name)
        
        # T√¨m trong DB b·∫±ng SLUG
        category = Category.query.filter_by(slug=category_slug).first()
        
        if not category:
            print(f"   Creating new category: {category_name} ({category_slug})")
            category = Category(name=category_name, slug=category_slug)
            db.session.add(category)
            db.session.flush()
        else:
            print(f"   Found existing category: {category.name}")


        # 4. Kh·ªüi t·∫°o Object Video
        video = Video(
            source_type='youtube',
            source_url=url,
            youtube_id=yt_id,
            title=meta['title'],
            thumbnail_url=meta['thumbnail'],
            category_id=category.id,
            level=level,
            slug = create_unique_slug(Video, meta['title']),
            added_by_user_id=user_id
        )
        db.session.add(video)
        db.session.flush()

        # 4. L∆∞u ph·ª• ƒë·ªÅ
        saved_count = 0
        for i, item in enumerate(transcript):
            if item['text']:
                db.session.add(Subtitle(
                    video_id=video.id,
                    start_time=item['start'],
                    end_time=item['start'] + item['duration'],
                    content_en=item['text'],
                    content_vi=transcript_vi[i]['text']
                ))
                saved_count += 1

        print(f"üíæ Saved {saved_count} subtitle entries")
        print(f"üìä Summary: EN={en_type}, VI={vi_type}\n")

        db.session.commit()
        return video

    except Exception as e:
        db.session.rollback()
        import traceback
        print(f"‚ùå Error: {str(e)}")
        print(traceback.format_exc())
        raise ValueError(f"L·ªói nh·∫≠p li·ªáu t·ª´ YouTube: {str(e)}")
def get_all_videos(page, per_page, category_id=None):
    query = Video.query
    if category_id:
        query = query.filter_by(category_id=category_id)

    paginated_result = query.paginate(page=page, per_page=per_page, error_out=False)

    return {
        "videos": VideoSchema(many=True).dump(paginated_result.items),
        "pagination": {
            "current_page": paginated_result.page,
            "total_items": paginated_result.total,
            "total_pages": paginated_result.pages,
            "has_next": paginated_result.has_next,
            "has_prev": paginated_result.has_prev
        }
    }