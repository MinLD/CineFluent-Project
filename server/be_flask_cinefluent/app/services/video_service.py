import os
import json
import tempfile
import time
import shutil
import glob
import traceback
import yt_dlp
from datetime import timedelta
from slugify import slugify
from deep_translator import GoogleTranslator

# Global Translator instance
translator = GoogleTranslator(source='en', target='vi')

def translate_with_retry(text, max_retries=3):
    """D·ªãch v·ªõi retry logic cho rate limiting"""
    for attempt in range(max_retries):
        try:
            return translator.translate(text)
        except Exception as e:
            error_str = str(e)
            if '429' in error_str or 'Too Many Requests' in error_str or 'rate' in error_str.lower():
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 2  # Exponential backoff: 2s, 4s, 8s
                    print(f"      ‚ö†Ô∏è Rate limited, waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"Rate limit exceeded after {max_retries} retries")
            else:
                raise
    return text

from ..models.models_model import Video, Subtitle, Category
from ..extensions import db, socketio
from ..schemas.video_schema import VideoSchema
from ..utils.subtitle_utils import parse_vtt
from .tmdb_service import search_movie_by_tmdb
from .learning_service import suggest_multiple_categories
from ..schemas.video_schema import (
    ImportYoutubeRequest, 
    ImportLocalRequest, 
    ImportLocalManualRequest,
    UpdateVideoRequest
)

# --- CONFIGURATION & PATHS ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
COOKIE_PATH = os.path.abspath(os.path.join(CURRENT_DIR, "../utils/www.youtube.com_cookies.txt"))

def fetch_youtube_metadata(url):
    """S·ª≠ d·ª•ng yt-dlp ƒë·ªÉ l·∫•y Title v√† Thumbnail t·ª´ link YouTube."""
    ydl_opts = {'quiet': True, 'no_warnings': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=False)
        return {
            'title': info.get('title'),
            'thumbnail': info.get('thumbnail'),
            'duration': info.get('duration'),
            'youtube_id': info.get('id'),
            'description': info.get('description')
        }
def delete_video_youtube(id):
    video = Video.query.get(id)
    if not video:
        raise ValueError('Video id {} not found'.format(id))
    db.session.delete(video)
    db.session.commit()
def create_unique_slug(model, base_title, max_length=100):
    base_slug = slugify(base_title)[:max_length]
    slug = base_slug
    counter = 1
    while model.query.filter_by(slug=slug).first():
        suffix = f"-{counter}"
        slug = f"{base_slug[:max_length - len(suffix)]}{suffix}"
        counter += 1

    return slug


def import_youtube_video(user_id, data: ImportYoutubeRequest):
    """
    H√†m n√†y ƒë∆∞·ª£c chuy·ªÉn th√†nh Generator ƒë·ªÉ yield th√¥ng b√°o ti·∫øn ƒë·ªô.
    """
    url = data.url
    yield {"status": "processing", "message": "ƒêang l·∫•y th√¥ng tin video t·ª´ YouTube...", "step": 1}
    meta = fetch_youtube_metadata(url)
    yt_id = meta['youtube_id']
    canonical_url = f"https://www.youtube.com/watch?v={yt_id}"

    # 1. X·ª≠ l√Ω Categories
    youtube_category = Category.query.filter_by(slug='youtube').first()
    if not youtube_category:
        youtube_category = Category(
            name='YouTube', 
            slug='youtube', 
            description='Danh m·ª•c t·ªïng h·ª£p c√°c video t·ª´ YouTube'
        )
        db.session.add(youtube_category)
        db.session.flush()
    
    categories = [youtube_category]
    if data.category_ids:
        additional_cats = Category.query.filter(Category.id.in_(data.category_ids)).all()
        for cat in additional_cats:
            if cat.id != youtube_category.id:
                categories.append(cat)

    # 2. Ki·ªÉm tra Video t·ªìn t·∫°i ch∆∞a (S·ª≠ d·ª•ng Canonical URL ho·∫∑c YouTube ID)
    video = Video.query.filter(
        (Video.source_url == canonical_url) | 
        (Video.source_url.contains(yt_id))
    ).first()

    if video:
        # C·∫≠p nh·∫≠t th√™m categories n·∫øu ch∆∞a c√≥
        for cat in categories:
            if cat not in video.categories:
                video.categories.append(cat)
        
        # N·∫øu URL c≈© kh√¥ng ph·∫£i canonical, c·∫≠p nh·∫≠t l·∫°i lu√¥n cho ƒë·∫πp
        if video.source_url != canonical_url:
            video.source_url = canonical_url
            
        db.session.commit()
        yield {"status": "completed", "message": "Video n√†y ƒë√£ t·ªìn t·∫°i trong h·ªá th·ªëng. ƒê√£ c·∫≠p nh·∫≠t danh m·ª•c.", "video_id": video.id}
        return

    try:
        # T·∫°o th∆∞ m·ª•c t·∫°m
        temp_dir = tempfile.mkdtemp()
        # C·∫•u h√¨nh yt-dlp
        ydl_opts = {
            'skip_download': True,
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['en', 'vi'],
            'subtitlesformat': 'json3',
            'outtmpl': os.path.join(temp_dir, '%(id)s.%(ext)s'),
            'quiet': True,
            'no_warnings': True,
        }

        if os.path.exists(COOKIE_PATH):
            ydl_opts['cookiefile'] = COOKIE_PATH
            print(f"Using cookies")

        # Download subtitles v√† l·∫•y metadata
        print(f"Downloading subtitles for: {url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            # Extract info ƒë·ªÉ l·∫•y metadata
            info = ydl.extract_info(url, download=False)

            # Ki·ªÉm tra subtitle metadata
            subtitles = info.get('subtitles', {})
            automatic_captions = info.get('automatic_captions', {})

            # Log th√¥ng tin subtitle
            print("\nSubtitle Information:")
            print(f"   Manual subtitles available: {list(subtitles.keys())}")
            print(f"   Auto-generated captions available: {list(automatic_captions.keys())}")

            # Ki·ªÉm tra ti·∫øng Anh
            if 'en' in subtitles:
                print(f"   English: MANUAL (human-created)")
                en_type = "manual"
            elif 'en' in automatic_captions:
                print(f"   English: AUTO-GENERATED")
                en_type = "auto"
            else:
                print(f"   English: NOT FOUND")
                en_type = "none"

            # Ki·ªÉm tra ti·∫øng Vi·ªát
            if 'vi' in subtitles:
                print(f"   Vietnamese: MANUAL (human-created)")
                vi_type = "manual"
            elif 'vi' in automatic_captions:
                print(f"   Vietnamese: AUTO-GENERATED")
                vi_type = "auto"
            else:
                print(f"   Vietnamese: NOT FOUND (will use auto-translate)")
                vi_type = "translated"

            print()  # Empty line for readability

            yield {"status": "processing", "message": f"ƒêang t·∫£i ph·ª• ƒë·ªÅ ({en_type}/{vi_type})...", "step": 2}
            # Download subtitles
            ydl.download([url])

        # Parse subtitle files
        def parse_json3_file(filepath):
            """Parse subtitle t·ª´ file json3"""
            if not os.path.exists(filepath):
                return []

            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            result = []
            for event in data.get('events', []):
                if 'segs' in event and event.get('segs'):
                    text = ''.join(seg.get('utf8', '') for seg in event['segs'])
                    if text.strip():
                        result.append({
                            'text': text.strip(),
                            'start': event.get('tStartMs', 0) / 1000.0,
                            'duration': event.get('dDurationMs', 0) / 1000.0
                        })
            return result

        # T√¨m file subtitle ƒë√£ download
        en_file = os.path.join(temp_dir, f"{yt_id}.en.json3")
        vi_file = os.path.join(temp_dir, f"{yt_id}.vi.json3")

        # T√¨m file subtitle ƒë√£ download
        if not os.path.exists(en_file):
            en_files = glob.glob(os.path.join(temp_dir, f"{yt_id}.en*.json3"))
            if en_files:
                en_file = en_files[0]

        if not os.path.exists(vi_file):
            vi_files = glob.glob(os.path.join(temp_dir, f"{yt_id}.vi*.json3"))
            if vi_files:
                vi_file = vi_files[0]

        print(f"Temp directory: {temp_dir}")
        print(f"English file: {en_file} (exists: {os.path.exists(en_file)})")
        print(f"Vietnamese file: {vi_file} (exists: {os.path.exists(vi_file)})")

        # Parse English subtitles
        transcript = parse_json3_file(en_file)
        if not transcript:
            raise ValueError("Kh√¥ng t√¨m th·∫•y ph·ª• ƒë·ªÅ ti·∫øng Anh")

        print(f"Got {len(transcript)} English entries ({en_type})")

        # D·ªãch subtitle b·∫±ng Google Translate API (BATCH MODE - T·ªëi ∆∞u ƒë·ªông)
        print("Translating subtitles using Google Translate (Dynamic Batch mode)...")
        
        # T√°i s·ª≠ d·ª•ng translator to√†n c·ª•c
        # T√≠nh batch size ƒë·ªông d·ª±a tr√™n ƒë·ªô d√†i text
        total_chars = sum(len(item['text']) for item in transcript)
        avg_chars = total_chars / len(transcript) if transcript else 50
        
        MAX_CHARS_PER_BATCH = 4500
        SEPARATOR = ' |||SUBTITLE_SEP||| '
        SEPARATOR_LENGTH = len(SEPARATOR)
        
        BATCH_SIZE = int(MAX_CHARS_PER_BATCH / (avg_chars + SEPARATOR_LENGTH))
        BATCH_SIZE = max(10, min(BATCH_SIZE, 200))
        
        transcript_vi = []
        total_batches = (len(transcript) + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"   üéØ Optimal batch size: {BATCH_SIZE}, Total batches: {total_batches}")
        yield {"status": "processing", "message": f"ƒêang d·ªãch {len(transcript)} c√¢u ph·ª• ƒë·ªÅ (chia l√†m {total_batches} ƒë·ª£t)...", "step": 3}
        
        for batch_idx in range(0, len(transcript), BATCH_SIZE):
            batch_num = (batch_idx // BATCH_SIZE) + 1
            batch = transcript[batch_idx:batch_idx + BATCH_SIZE]
            
            # T√≠nh s·ªë k√Ω t·ª± trong batch n√†y
            batch_chars = sum(len(item['text']) for item in batch)
            
            try:
                # G·ªôp t·∫•t c·∫£ c√¢u trong batch th√†nh 1 string
                combined_text = SEPARATOR.join([item['text'] for item in batch])
                
                print(f"   üì¶ Batch {batch_num}/{total_batches}: {len(batch)} subtitles ({batch_chars} chars)...")
                
                # D·ªãch 1 l·∫ßn cho c·∫£ batch v·ªõi retry
                translated_combined = translate_with_retry(combined_text)
                
                # T√°ch l·∫°i th√†nh c√°c c√¢u ri√™ng l·∫ª
                translated_parts = translated_combined.split('|||SUBTITLE_SEP|||')
                
                # Clean up whitespace
                translated_parts = [part.strip() for part in translated_parts]
                
                # Ki·ªÉm tra s·ªë l∆∞·ª£ng c√≥ kh·ªõp kh√¥ng
                if len(translated_parts) != len(batch):
                    print(f"      ‚ö†Ô∏è Warning: Expected {len(batch)} parts, got {len(translated_parts)}")
                    # Fallback: d·ªãch t·ª´ng c√¢u n·∫øu batch translation fail
                    print(f"      üîÑ Falling back to single-sentence translation...")
                    translated_parts = []
                    for item in batch:
                        try:
                            vi_text = translate_with_retry(item['text'])
                            translated_parts.append(vi_text)
                        except:
                            translated_parts.append('')
                
                # Th√™m v√†o k·∫øt qu·∫£
                for i, item in enumerate(batch):
                    vi_text = translated_parts[i] if i < len(translated_parts) else ''
                    transcript_vi.append({
                        'text': vi_text,
                        'start': item['start'],
                        'duration': item['duration']
                    })
                
                print(f"      ‚úÖ Batch {batch_num}/{total_batches} completed!")
                yield {"status": "processing", "message": f"ƒê√£ d·ªãch xong {min(batch_idx + BATCH_SIZE, len(transcript))}/{len(transcript)} c√¢u...", "step": 3}
                
                # Th√™m delay nh·ªè gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limit
                if batch_num < total_batches:
                    time.sleep(0.5)
                
            except Exception as e:
                print(f"      ‚ùå Batch {batch_num} failed: {str(e)}")
                print(f"      üîÑ Falling back to single-sentence translation...")
                
                # Fallback: d·ªãch t·ª´ng c√¢u
                for item in batch:
                    try:
                        vi_text = translate_with_retry(item['text'])
                    except:
                        vi_text = ''
                    
                    transcript_vi.append({
                        'text': vi_text,
                        'start': item['start'],
                        'duration': item['duration']
                    })
                    time.sleep(0.3)  # Delay nh·ªè gi·ªØa c√°c c√¢u
        
        print(f"‚úÖ Translated {len(transcript_vi)}/{len(transcript)} subtitles successfully (Dynamic Batch mode)")

        # Cleanup temp files
        shutil.rmtree(temp_dir, ignore_errors=True)

        # 3. Kh·ªüi t·∫°o Object Video
        raw_title = meta['title']
        translated_title = translate_with_retry(raw_title)
        
        video = Video(
            source_type='youtube',
            source_url=canonical_url,
            original_title=raw_title,
            title=translated_title,
            description=meta.get('description'),
            thumbnail_url=meta['thumbnail'],
            level=data.level,
            slug = create_unique_slug(Video, translated_title), # D√πng title d·ªãch cho slug
            author = meta.get('author'),
            country = meta.get('country'),
            status = data.status
        )
        # G√°n danh s√°ch categories (N-N)
        video.categories = categories
        db.session.add(video)
        db.session.flush()

        # 4. L∆∞u ph·ª• ƒë·ªÅ
        saved_count = 0
        for i, item in enumerate(transcript):
            if item['text']:
                db.session.add(Subtitle(
                    video_id=video.id,
                    start_time=item['start'],
                    end_time=item['start'] + item['duration'],
                    content_en=item['text'],
                    content_vi=transcript_vi[i]['text']
                ))
                saved_count += 1

        print(f"üíæ Saved {saved_count} subtitle entries")
        print(f"üìä Summary: EN={en_type}, VI={vi_type}\n")

        db.session.commit()
        
        # [VTT_OPTIMIZATION] T·ª± ƒë·ªông xu·∫•t file VTT ngay sau khi l∆∞u DB th√†nh c√¥ng
        yield {"status": "processing", "message": "ƒêang kh·ªüi t·∫°o file ph·ª• ƒë·ªÅ VTT t·ªëi ∆∞u...", "step": 4}
        export_subtitle_to_vtt(video.id)

        yield {"status": "completed", "message": "Ho√†n t·∫•t! Video ƒë√£ s·∫µn s√†ng tr√™n h·ªá th·ªëng.", "video_id": video.id}
        return

    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error: {str(e)}")
        print(traceback.format_exc())
        print(traceback.format_exc())
        raise ValueError(f"L·ªói nh·∫≠p li·ªáu t·ª´ YouTube: {str(e)}")

def import_local_video_by_tmdb(user_id, data: ImportLocalRequest): 
    tmdb_id = data.tmdb_id
    movie = search_movie_by_tmdb(tmdb_id)       
    if not movie:
        raise ValueError(f"Kh√¥ng t√¨m th·∫•y phim v·ªõi TMDB ID {tmdb_id}")

    # 0. Ki·ªÉm tra Video ƒë√£ t·ªìn t·∫°i theo TMDB ID ch∆∞a
    if Video.query.filter_by(tmdb_id=tmdb_id).first():
        raise ValueError(f"Phim v·ªõi TMDB ID {tmdb_id} ƒë√£ t·ªìn t·∫°i")

    
    # 1. AI g·ª£i √Ω Categories d·ª±a tr√™n Ti√™u ƒë·ªÅ v√† NƒÉm
    ai_res = suggest_multiple_categories(movie['title'], movie['year'])
    category_names = ai_res.get("data", []) if ai_res.get("success") else ["Phim"]
    
    # Chu·∫©n h√≥a t√™n Category (Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu)
    categories = []
    for name in category_names:
        name = name.strip().title()
        slug = slugify(name)
        if not slug: continue
        
        # T√¨m ho·∫∑c t·∫°o m·ªõi Category
        category = Category.query.filter_by(slug=slug).first()
        if not category:
            category = Category(
                name=name,
                slug=slug,
                description=f"Phim thu·ªôc th·ªÉ lo·∫°i {name}"
            )
            db.session.add(category)
            db.session.flush() # L·∫•y ID ngay l·∫≠p t·ª©c
        
        if category not in categories:
            categories.append(category)

    # 2. T·∫°o ƒë·ªëi t∆∞·ª£ng Video m·ªõi n·∫øu ch∆∞a c√≥
    video = Video(
        source_type='local',
        tmdb_id=tmdb_id,
        title=movie['title'],
        original_title=movie.get('original_title'),
        description=movie['overview'],
        author=movie.get('author') or "Unknown",
        country=movie.get('country') or "Unknown",
        release_year=movie.get('year'),
        thumbnail_url=movie['poster_path'],
        backdrop_url=movie['backdrop_path'],
        level='Intermediate', 
        slug=create_unique_slug(Video, movie['title']),
        view_count=0,
        runtime=movie.get('runtime'),
        status='private' # M·∫∑c ƒë·ªãnh l√† private ƒë·ªÉ admin duy·ªát tr∆∞·ªõc
    )
    video.categories = categories # G√°n quan h·ªá n-n
    
    db.session.add(video)
    db.session.commit()
 
    return video

def import_video_local(user_id, data: ImportLocalManualRequest):
    # 1. X·ª≠ l√Ω Categories t·ª´ danh s√°ch ID
    categories = []
    if data.category_ids:
        categories = Category.query.filter(Category.id.in_(data.category_ids)).all()

    # 2. T·∫°o ƒë·ªëi t∆∞·ª£ng Video b·∫±ng c√°ch truy c·∫≠p tr·ª±c ti·∫øp thu·ªôc t√≠nh c·ªßa Model
    video = Video(
        tmdb_id=data.tmdb_id,
        source_type='local',
        title=data.title,
        original_title=data.original_title,
        description=data.description,
        author=data.author or "Unknown",
        country=data.country or "Unknown",
        release_year=data.release_year,
        thumbnail_url=data.thumbnail_url,
        backdrop_url=data.backdrop_url,
        runtime=data.runtime,
        level=data.level,
        status=data.status,
        slug=create_unique_slug(Video, data.title),
        view_count=0
    )
    
    # G√°n categories (N-N)
    if categories:
        video.categories = categories
    
    db.session.add(video)
    db.session.commit()

    return video

def save_subtitles_from_content(video_id, en_content, vi_content=None):
    """
    Parse v√† l∆∞u ph·ª• ƒë·ªÅ t·ª´ n·ªôi dung text (SRT ho·∫∑c VTT) v√†o database.
    T·ª± ƒë·ªông nh·∫≠n di·ªán ƒë·ªãnh d·∫°ng d·ª±a tr√™n n·ªôi dung.
    """
    from ..utils.subtitle_utils import parse_vtt, parse_srt
    
    video = Video.query.get(video_id)
    if not video:
        raise ValueError(f"Video ID {video_id} kh√¥ng t·ªìn t·∫°i")

    # X√≥a sub c≈©
    Subtitle.query.filter_by(video_id=video_id).delete()

    # Nh·∫≠n di·ªán ƒë·ªãnh d·∫°ng cho file Anh
    if en_content:
        parser_en = parse_vtt if 'WEBVTT' in en_content.upper() else parse_srt
        en_subs = parser_en(en_content)
    else:
        en_subs = []
    
    # Nh·∫≠n di·ªán ƒë·ªãnh d·∫°ng cho file Vi·ªát
    vi_subs = []
    if vi_content:
        parser_vi = parse_vtt if 'WEBVTT' in vi_content.upper() else parse_srt
        vi_subs = parser_vi(vi_content)

    # S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o logic so s√°nh th·ªùi gian ch√≠nh x√°c
    en_subs.sort(key=lambda x: x['start_time'])
    vi_subs.sort(key=lambda x: x['start_time'])
    # N·∫øu ng∆∞·ªùi d√πng ch·ªâ upload file Vi·ªát, d√πng file Vi·ªát l√†m tr·ª•c ch√≠nh
    if not en_subs and vi_subs:
        en_subs = vi_subs
        vi_subs = []

    count = 0
    en_midpoints = [(sub['start_time'] + sub['end_time']) / 2.0 for sub in en_subs]
    vi_matched_texts = [[] for _ in en_subs]

    en_idx = 0
    num_en = len(en_subs)

    for vi_sub in vi_subs:
        if num_en == 0:
            break
            
        vi_mid = (vi_sub['start_time'] + vi_sub['end_time']) / 2.0
        
        # Ti·∫øn en_idx ƒë·ªÉ t√¨m midpoint g·∫ßn nh·∫•t
        while en_idx < num_en - 1:
            dist_curr = abs(en_midpoints[en_idx] - vi_mid)
            dist_next = abs(en_midpoints[en_idx + 1] - vi_mid)
            if dist_next <= dist_curr:
                en_idx += 1
            else:
                break
        
        if en_idx < num_en:
            # Ng∆∞·ª°ng 10 gi√¢y (n·∫øu l·ªách qu√° 10s so v·ªõi c√¢u EN g·∫ßn nh·∫•t th√¨ b·ªè qua)
            if abs(en_midpoints[en_idx] - vi_mid) <= 10.0:
                vi_matched_texts[en_idx].append(vi_sub['text'])

    for i, en_sub in enumerate(en_subs):
        best_vi_text = " ".join(vi_matched_texts[i])

        # Tr∆∞·ªùng h·ª£p ch·ªâ c√≥ 1 file (ƒë√£ map vi_subs sang en_subs ·ªü tr√™n)
        if not en_content and vi_content:
            en_text = ""
            vi_text = en_sub['text']
        else:
            en_text = en_sub['text']
            vi_text = best_vi_text

        new_sub = Subtitle(
            video_id=video_id,
            start_time=en_sub['start_time'],
            end_time=en_sub['end_time'],
            content_en=en_text,
            content_vi=vi_text
        )
        db.session.add(new_sub)
        count += 1

    db.session.commit()
    
    # Xu·∫•t ra file VTT t·ªïng h·ª£p cho Frontend
    export_subtitle_to_vtt(video_id)
    
    return count


def get_all_videos(page, per_page, category_id=None, release_year=None, source_type=None, status='public', keyword=None):
    query = Video.query
    
    if status and status != 'all':
        query = query.filter_by(status=status)
    
    if keyword:
        # T√¨m ki·∫øm theo ti√™u ƒë·ªÅ (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        query = query.filter(Video.title.ilike(f"%{keyword}%"))

    if category_id:
        # L·ªçc theo Many-to-Many
        query = query.filter(Video.categories.any(id=category_id))
        
    if release_year:
        query = query.filter(Video.release_year == release_year)
    
    if source_type and source_type in ['youtube', 'local']:
        query = query.filter_by(source_type=source_type)
        
    query = query.order_by(Video.id.desc())

    paginated_result = query.paginate(page=page, per_page=per_page, error_out=False)

    return {
        "videos": VideoSchema(many=True).dump(paginated_result.items),
        "pagination": {
            "current_page": paginated_result.page,
            "total_items": paginated_result.total,
            "total_pages": paginated_result.pages,
            "has_next": paginated_result.has_next,
            "has_prev": paginated_result.has_prev
        }
    }

def get_video_by_id(video_id):
    return Video.query.get(video_id)

def update_video(video_id, data: UpdateVideoRequest):
    video = Video.query.get(video_id)
    if not video:
        raise ValueError('Kh√¥ng t√¨m th·∫•y video')

    # Chuy·ªÉn Model th√†nh dict, lo·∫°i b·ªè c√°c gi√° tr·ªã None (Ch·ªâ update nh·ªØng g√¨ ƒë∆∞·ª£c g·ª≠i)
    update_data = data.model_dump(exclude_unset=True)

    if 'title' in update_data:
        video.title = update_data['title']
        video.slug = create_unique_slug(Video, update_data['title'])
    
    if 'category_ids' in update_data:
        categories = Category.query.filter(Category.id.in_(update_data['category_ids'])).all()
        video.categories = categories

    # T·ª± ƒë·ªông c·∫≠p nh·∫≠t c√°c field c√≤n l·∫°i n·∫øu c√≥ trong Model
    for field, value in update_data.items():
        if field not in ['title', 'category_ids'] and hasattr(video, field):
            setattr(video, field, value)

    try:
        db.session.commit()
        return video
    except Exception as e:
        db.session.rollback()
        raise e

def delete_all_subtitles(video_id):
    """X√≥a to√†n b·ªô ph·ª• ƒë·ªÅ c·ªßa m·ªôt video."""
    video = Video.query.get(video_id)
    if not video:
        raise ValueError(f"Video ID {video_id} kh√¥ng t·ªìn t·∫°i")
    
    Subtitle.query.filter_by(video_id=video_id).delete()
    video.subtitle_vtt_url = None
    db.session.commit()
    
    # X√≥a file v·∫≠t l√Ω n·∫øu c√≥
    storage_dir = os.path.abspath(os.path.join(os.getcwd(), 'storage', 'subtitles'))
    file_path = os.path.join(storage_dir, f"video_{video_id}.vtt")
    if os.path.exists(file_path):
        os.remove(file_path)
        
    return True

# [VTT_OPTIMIZATION] Cung c·∫•p h√†m xu·∫•t sub ra file v·∫≠t l√Ω ƒë·ªÉ tƒÉng hi·ªáu nƒÉng
def export_subtitle_to_vtt(video_id):
    import os
    from datetime import timedelta
    from ..models.models_model import Video, Subtitle

    video = Video.query.get(video_id)
    if not video:
        raise ValueError(f"Video ID {video_id} kh√¥ng t·ªìn t·∫°i")

    # 1. T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a c√≥
    storage_dir = os.path.abspath(os.path.join(os.getcwd(), 'storage', 'subtitles'))
    os.makedirs(storage_dir, exist_ok=True)
    
    file_name = f"video_{video_id}.vtt"
    file_path = os.path.join(storage_dir, file_name)

    # 2. L·∫•y danh s√°ch ph·ª• ƒë·ªÅ s·∫Øp x·∫øp theo th·ªùi gian
    subs = Subtitle.query.filter_by(video_id=video_id).order_by(Subtitle.start_time.asc()).all()
    
    def format_vtt_time(seconds):
        td = timedelta(seconds=seconds)
        total_seconds = int(td.total_seconds())
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        secs = total_seconds % 60
        millis = int((td.total_seconds() - total_seconds) * 1000)
        return f"{hours:02}:{minutes:02}:{secs:02}.{millis:03}"

    # 3. Ghi file theo chu·∫©n WebVTT
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("WEBVTT\n\n")
        for i, sub in enumerate(subs):
            start = format_vtt_time(sub.start_time)
            end = format_vtt_time(sub.end_time)
            # Ghi c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát (t√πy nhu c·∫ßu UI, ·ªü ƒë√¢y ta g·ªôp ho·∫∑c t√°ch t√πy √Ω)
            # Frontend th∆∞·ªùng c·∫ßn text ƒë∆°n thu·∫ßn, ta c√≥ th·ªÉ format ƒë·∫∑c bi·ªát n·∫øu mu·ªën
            f.write(f"{i+1}\n")
            f.write(f"{start} --> {end}\n")
            # Encode n·ªôi dung ti·∫øng Vi·ªát r√†nh m·∫°ch
            content = sub.content_en
            if sub.content_vi:
                content += f"\n{sub.content_vi}"
            f.write(f"{content}\n\n")

    # 4. C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n v√†o Database (URL t∆∞∆°ng ƒë·ªëi cho Frontend)
    # Kh√¥ng ƒë·ªÉ /api ·ªü ƒë·∫ßu v√¨ s·∫Ω b·ªã nh√¢n ƒë√¥i khi quan qua Proxy/API_BASE_URL
    video.subtitle_vtt_url = f"/static/subs/{file_name}"
    db.session.commit()
    
    print(f"‚úÖ Exported VTT for Video {video_id}: {file_path}")
    return video.subtitle_vtt_url
